# -*- coding: utf-8 -*-
"""Autism Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wdyw-iqD06GDylUs5qMDfiB1Jc8SomYi
"""

#Since column name same data is merged
import pandas as pd

# Load the datasets
dataset1 = pd.read_csv('/content/train.csv')
dataset2 = pd.read_csv('/content/autism_screening.csv')

# Fix column names (standardize 'autism' column)
dataset1.rename(columns={'austim': 'autism'}, inplace=True)
dataset2.rename(columns={'austism': 'autism'}, inplace=True)

# Drop 'ID' column from dataset1 since dataset2 doesn’t have it
dataset1.drop(columns=['ID'], inplace=True)

# Confirm matching columns before merging
assert list(dataset1.columns) == list(dataset2.columns), "Column mismatch after renaming!"

# Combine datasets
df = pd.concat([dataset1, dataset2], ignore_index=True)

# Save combined dataset
df.to_csv('autism.csv', index=False)

print("Datasets merged and saved as 'autism.csv'")

#Get generic info of dataset
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/autism.csv')

# Display basic info and first few rows
print(df.info())
print(df.head())

# Check for null values in each column
print(df.isnull().sum())

# Drop Duplicates
import pandas as pd

# Load dataset
df = pd.read_csv('/content/autism.csv')

# Remove duplicate rows from the entire dataset
df.drop_duplicates(inplace=True)

# Save the cleaned dataset back to the same file
df.to_csv('autism.csv', index=False)

# Check for missing and ’?’ values:
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/autism.csv')

# Replace '?' with NaN for consistency
df.replace('?', pd.NA, inplace=True)

# Count missing (NaN) values in each column
missing_values = df.isna().sum()

# Count '?' in each column (if any)
unknown_values = (df == '?').sum()

# Combine missing and unknown counts
missing_and_unknown = missing_values + unknown_values

# Display the result
print("Missing or Unknown values in each column:")
print(missing_and_unknown)

# Replace ? with ‘unknown’:
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/autism.csv')

# Replace '?' with NaN for consistency
df.replace('?', pd.NA, inplace=True)

# Replace both NaN and NaN-like values ('?') with 'Unknown'
df.fillna('Unknown', inplace=True)

# Save the cleaned dataset back to the same file
df.to_csv('autism.csv', index=False)

# Download the cleaned dataset
from google.colab import files
files.download('autism.csv')

# Check -ve values of age and result  in dataset amd drop thwm:
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/autism.csv')

# Convert 'age' and 'result' to numeric (in case they are strings)
df['age'] = pd.to_numeric(df['age'], errors='coerce')
df['result'] = pd.to_numeric(df['result'], errors='coerce')

# Check how many values are less than 0
negative_age_count = (df['age'] < 0).sum()
negative_result_count = (df['result'] < 0).sum()

print(f"Negative values in 'age': {negative_age_count}")
print(f"Negative values in 'result': {negative_result_count}")

# Drop rows where 'age' or 'result' is less than 0
df = df[(df['age'] >= 0) & (df['result'] >= 0)]

# Save the cleaned dataset
df.to_csv('autism.csv', index=False)

# Download the updated dataset
from google.colab import files
files.download('autism.csv')

# Convert age and result column data into whole numbers and then integers and save into autism.csv:
import pandas as pd

# Load dataset
df = pd.read_csv('/content/autism.csv')

# Drop unwanted columns
df.drop(['ethnicity', 'age_desc'], axis=1, inplace=True)

# Round 'age' and 'result' to nearest whole number
df['age'] = df['age'].round()
df['result'] = df['result'].round()

# Drop rows where 'age' or 'result' is missing (NaN)
df.dropna(subset=['age', 'result'], inplace=True)

# Convert to integers
df['age'] = df['age'].astype(int)
df['result'] = df['result'].astype(int)

# Save and download
df.to_csv('autism.csv', index=False)

from google.colab import files
files.download('autism.csv')

# Entropy calculation:
import pandas as pd
import numpy as np
from scipy.stats import entropy

# Load dataset
df = pd.read_csv('/content/autism.csv')

# Function to calculate entropy of a single column
def column_entropy(series):
    counts = series.value_counts(dropna=False)
    probabilities = counts / counts.sum()
    return entropy(probabilities, base=2)  # base=2 gives entropy in bits

# Calculate entropy for each column
entropy_dict = {}
for col in df.columns:
    try:
        entropy_dict[col] = column_entropy(df[col])
    except Exception as e:
        entropy_dict[col] = f"Error: {e}"

# Display entropy of each column
for col, ent in entropy_dict.items():
    print(f"{col}: {ent}")

# Information gain:
import pandas as pd
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv("/content/autism.csv")

# Separate features and target
X = df.drop("Class/ASD", axis=1)
y = df["Class/ASD"]

# Ensure all categorical columns are encoded
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = LabelEncoder().fit_transform(X[col])

# Calculate mutual information (information gain)
info_gain = mutual_info_classif(X, y, discrete_features='auto')

# Display results
info_gain_df = pd.DataFrame({
    "Feature": X.columns,
    "Information Gain": info_gain
}).sort_values(by="Information Gain", ascending=False)

print(info_gain_df)

# Install xgboost:
!pip install xgboost

# Step 1: Preprocess the Data (Encoding Categorical Columns) for simple yes and no I used find and replace to change yes into 1 and no into 0
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('/content/half_coded_autism.csv')

# # Step 1: Check unique values in 'Class/ASD' column to understand the data better
# print(df['Class/ASD'].unique())

# # Step 2: Clean and standardize the 'Class/ASD' column (lowercase and strip spaces)
# df['Class/ASD'] = df['Class/ASD'].str.lower().str.strip()

# # Step 3: Check again for unique values after standardization
# print(df['Class/ASD'].unique())

# # Step 4: Map 'yes' to 1 and 'no' to 0 in 'Class/ASD', only if the value is 'yes' or 'no'
# df['Class/ASD'] = df['Class/ASD'].map({'yes': 1, 'no': 0})

# # Ensure that the mapping was applied correctly, check for any missing or unexpected values
# print(df['Class/ASD'].unique())

# Step 5: Encode other categorical columns (if any)
categorical_columns = ['gender', 'jaundice', 'autism', 'contry_of_res', 'used_app_before', 'result', 'relation']

label_encoder = LabelEncoder()

for col in categorical_columns:
    # Convert to string if not already a string type
    df[col] = df[col].astype(str)
    df[col] = df[col].str.lower().str.strip()  # Standardize the column
    df[col] = label_encoder.fit_transform(df[col])  # Apply label encoding

# Step 6: Save the updated dataframe to a new CSV
df.to_csv('autism.csv', index=False)

# Step 7: Provide a link to download the updated file
from google.colab import files
files.download('autism.csv')

# Model training:
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
import joblib

# Load cleaned and encoded dataset
df = pd.read_csv('/content/autism.csv')

# Selected 15 features
selected_features = [
    'A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
    'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score',
    'age', 'result', 'autism', 'jaundice', 'relation'
]

# Input and target
X = df[selected_features]
y = df['Class/ASD']

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Define models
models = {
    'KNN': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    'Logistic Regression': LogisticRegression(max_iter=1000)
}

# Train, predict, and collect results
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)

    results.append({
        'Model': name,
        'Accuracy': round(acc, 4),
        'Precision': round(prec, 4),
        'Recall': round(rec, 4),
        'No. of Features': len(selected_features)
    })

# Save the trained XGBoost model
joblib.dump(models['XGBoost'], 'xgb_model.pkl')

# Convert to DataFrame and print
results_df = pd.DataFrame(results)
print(results_df)

# Install necessary libraries
!pip install lime

import pandas as pd
import numpy as np
import joblib
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt

# Load cleaned and encoded dataset
df = pd.read_csv('autism.csv')

# Selected 15 features
selected_features = [
    'A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
    'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score',
    'age', 'result', 'autism', 'jaundice', 'relation'
]

# Input and target
X = df[selected_features]
y = df['Class/ASD']

# Load the saved XGBoost model
model = joblib.load('xgb_model.pkl')

# Initialize LIME explainer
explainer = LimeTabularExplainer(
    training_data=np.array(X),
    feature_names=selected_features,
    class_names=['No Autism', 'Autism'],
    discretize_continuous=True
)

# Select an instance to explain
instance = X.iloc[0]

# Explain the instance prediction
explanation = explainer.explain_instance(
    instance.values,
    model.predict_proba,
    num_features=10
)

# Show the explanation
explanation.show_in_notebook()

# Optionally, you can also plot the explanation
explanation.as_pyplot_figure()
plt.show()